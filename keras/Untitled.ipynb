{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from CustomCallbacks import *\n",
    "import pickle\n",
    "from models import mini_XCEPTION, tiny_XCEPTION\n",
    "from data import load_emotion_data, split_data\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "from GenericNetworkBuilder import *\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "def SetUpDataGenerators(train_path, val_path, batch_size, w, h):\n",
    "    # batch_size = 16\n",
    "\n",
    "    training_datagen =ImageDataGenerator(featurewise_center=False,\n",
    "                                    featurewise_std_normalization=False,\n",
    "                                    rotation_range=10,\n",
    "                                    width_shift_range=0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    zoom_range=.1,\n",
    "                                    horizontal_flip=True)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator()\n",
    "\n",
    "    training_generator = training_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(w, h),\n",
    "        # color_mode='grayscale',\n",
    "        batch_size=batch_size\n",
    "\n",
    "    )\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        target_size=(w, h),\n",
    "        # color_mode='grayscale',\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return training_generator, validation_generator\n",
    "\n",
    "def load_data(root, split, input_shape, classes):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    print('Read train images')\n",
    "\n",
    "#     classes = glob.glob(root+'*')\n",
    "#     print(classes)\n",
    "\n",
    "    for j in range(0, len(classes)):\n",
    "        print('Load folder {}'.format(j))\n",
    "\n",
    "        files = glob.glob(root+classes[j]+'/*')\n",
    "        print(len(files))\n",
    "\n",
    "        # val_images = math.floor( len(files) * split )\n",
    "\n",
    "        # for i in range(0, val_images):\n",
    "\n",
    "        #     rand_idx = random.randint(0, len(files)-1)\n",
    "\n",
    "        #     fl = files[rand_idx]\n",
    "\n",
    "        #    files.pop(rand_idx)\n",
    "\n",
    "        #    img = get_im(fl, input_shape)\n",
    "        #    X_val.append(img)\n",
    "        #    y_val.append(j)\n",
    "\n",
    "        for fl in files:\n",
    "            img = get_im(fl, input_shape)\n",
    "            X_train.append(img)\n",
    "            y_train.append(j)\n",
    "\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = split_validation_set(np.array(X_train), np.array(y_train), split)\n",
    "\n",
    "    print(\"Using {} imgs for training and {} imgs for validation\".format( len(X_train), len(X_val) ) )\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def get_im(path, input_shape):\n",
    "    # Load as grayscale\n",
    "    # print(path)\n",
    "    try:\n",
    "        img = cv2.imread(path)\n",
    "\n",
    "        # Reduce size\n",
    "        resized = cv2.resize(img, (input_shape[0], input_shape[1]) )\n",
    "        if(input_shape[2] == 1):\n",
    "            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        resized = ((resized/255.0) - 0.5)*2.0\n",
    "    except:\n",
    "        print(\"something is off with {}\".format(path))\n",
    "        return\n",
    "\n",
    "\n",
    "    return resized\n",
    "\n",
    "def split_validation_set(train, labels, test_size):\n",
    "    random_state = 51\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# parameters\n",
    "batch_size = 32# * 4\n",
    "num_epochs = 10000\n",
    "\n",
    "conv2d_filters = [2, 4, 8, 12, 16, 32]\n",
    "# input_shapes = [(32, 32, 3), (48, 48, 3), (64, 64, 3), (80, 80, 3)]\n",
    "input_shape = (64, 64, 3)\n",
    "residual_layers = range(1,10)\n",
    "conv_layers = range(1,10)\n",
    "\n",
    "validation_split = .2\n",
    "verbose = 1\n",
    "num_classes = 4\n",
    "patience = 50\n",
    "gestures = ['nohand','peace','stop','thumbsup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder 0\n",
      "11350\n",
      "Load folder 1\n",
      "18633\n",
      "Load folder 2\n",
      "9540\n",
      "Load folder 3\n",
      "19788\n",
      "Using 47448 imgs for training and 11863 imgs for validation\n",
      "Read train images\n",
      "Load folder 0\n",
      "1500\n",
      "Load folder 1\n",
      "1500\n",
      "Load folder 2\n",
      "1000\n",
      "Load folder 3\n",
      "1500\n",
      "Using 5500 imgs for training and 0 imgs for validation\n"
     ]
    }
   ],
   "source": [
    "root = '/Users/jmarcano/dev/withme/HandGesturesAndTracking/images/CommonHandGestures/training_data/'\n",
    "\n",
    "root_test = '/Users/jmarcano/dev/withme/HandGesturesAndTracking/images/CommonHandGestures/testing_data/'\n",
    "x_train, y_train, x_val, y_val = load_data(root, 0.2, input_shape, gestures )\n",
    "\n",
    "# x_test, y_test, dummy1, dummy2 = load_data(root_test, 0.0, input_shape, gestures)\n",
    "\n",
    "y_train2 = np.zeros( (47448, 4), dtype=np.float64)\n",
    "y_val2 = np.zeros( (11863, 4), dtype=np.float64)\n",
    "\n",
    "for i in range(0, len(y_train)):\n",
    "    y_train2[i][ y_train[i]] =1\n",
    "\n",
    "for i in range(0, len(y_val)):\n",
    "    y_val2[i][ y_val[i] ] =1\n",
    "# fdata = open('train_val_data_{}_{}_{}.pickle'.format(input_shape[0], input_shape[1], input_shape[2]), 'wb')\n",
    "\n",
    "# pickle.dump([x_train, y_train, x_val, y_val], fdata)\n",
    "\n",
    "# fdata.close()\n",
    "\n",
    "# fdata2 = open('test_{}_{}_{}.pickle'.format(input_shape[0], input_shape[1], input_shape[2]), 'wb')\n",
    "\n",
    "\n",
    "# pickle.dump([x_test, y_test], fdata2)\n",
    "\n",
    "# fdata2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 62, 62, 4)    108         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 62, 62, 4)    16          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 62, 62, 4)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 60, 60, 4)    144         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 60, 60, 4)    16          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 60, 60, 4)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 58, 58, 4)    144         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 58, 58, 4)    16          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 58, 58, 4)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 56, 56, 4)    144         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 56, 56, 4)    16          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 56, 56, 4)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 56, 56, 8)    68          activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 56, 56, 8)    32          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 8)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 56, 56, 8)    136         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 56, 56, 8)    32          separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 8)    32          activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 8)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 28, 8)    32          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 28, 28, 8)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 28, 28, 16)   200         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 28, 28, 16)   64          separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 28, 28, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 28, 28, 16)   400         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 28, 28, 16)   64          separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 14, 14, 16)   128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 16)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 14, 14, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 14, 14, 16)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 14, 14, 24)   528         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 14, 14, 24)   96          separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 14, 14, 24)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 14, 14, 24)   792         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 14, 14, 24)   96          separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 7, 7, 24)     384         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 24)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 7, 7, 24)     96          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 7, 7, 24)     0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 7, 7, 4)      868         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 4)            0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 4)            0           global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 4,716\n",
      "Trainable params: 4,396\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_layer =3\n",
    "residual_layer=3\n",
    "conv2d_filter=8\n",
    "print(\"a\")\n",
    "now = datetime.datetime.now()\n",
    "base_path = 'arch_eval2/'\n",
    "base_path += now.strftime(\"%Y_%m_%d_%H_%M_%S\")+'/'\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "# datasets_path = 'data/fer2013/fer2013.csv'\n",
    "model_name = \"ResidualNet_{}x{}x{}_conv_{}_res{}\".format(input_shape[0], input_shape[1], input_shape[2],\n",
    "                                                         6, 6)\n",
    "\n",
    "model = ResidualNet(input_shape, num_classes, residual_layer, conv_layer, conv2d_filters=conv2d_filter)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# begin training\n",
    "log_file_path = base_path + model_name+'_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
    "                              patience=int(patience/4), verbose=1)\n",
    "trained_models_path = base_path + model_name\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "# model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "model_checkpoint = ModelCheckpointConfusion(model_names, 'val_acc', verbose=1,\n",
    "                                save_best_only=True, gestures=gestures, test_split= (x_test, y_test) )\n",
    "\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47448 samples, validate on 11863 samples\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train,\n",
    "                    y=y_train2,\n",
    "                    epochs=2, verbose=2,\n",
    "                    validation_data=(x_val, y_val2),\n",
    "                    steps_per_epoch=len(x_train) // batch_size,\n",
    "                    validation_steps=len(x_val) // batch_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47448 11863\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0 in y_train) and (1 in y_train) and (2 in y_train) and (3 in y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
