{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "muct_data_path = '/Users/azhong/face/clmtools/pdm_builder/data/images/'\n",
    "muct_annotation_path = '/Users/azhong/face/clmtools/pdm_builder/data/annotations.csv'\n",
    "\n",
    "landmark_dict = {}\n",
    "raw_image_dict = {}\n",
    "image_dict = {}\n",
    "image_array = []\n",
    "landmarks_array = []\n",
    "image_dim_dict = {}\n",
    "mouth_landmarks = range(44, 62)\n",
    "\n",
    "with open(muct_annotation_path) as fi:\n",
    "    for line in fi:\n",
    "        splitted = line.split(';')\n",
    "        filename = os.path.join(muct_data_path, splitted[0])\n",
    "        if os.path.isfile(filename):\n",
    "            landmark_dict[splitted[0]] = [[float(splitted[i*3+1]), float(splitted[i*3+2])] for i in range(71)]\n",
    "            landmark_dict[splitted[0]] = np.array(landmark_dict[splitted[0]])\n",
    "            raw_image_dict[splitted[0]] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2GRAY)\n",
    "            image_dim_dict[splitted[0]] = raw_image_dict[splitted[0]].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "augment_count = 10\n",
    "random.seed()\n",
    "for i in sorted(landmark_dict.keys()):\n",
    "    x2, y2 = np.amax(landmark_dict[i], axis=0)\n",
    "    x1, y1 = np.amin(landmark_dict[i], axis=0)\n",
    "    x1 = int(x1)\n",
    "    x2 = int(x2)\n",
    "    # extend up further to forehead\n",
    "    y1 = max(int(y1 - (y2-y1)*0.2), 0)\n",
    "    y2 = int(y2)\n",
    "    for j in range(augment_count):\n",
    "        # randomly translate in x and y direction by 10%\n",
    "        translate_range_x = (x2-x1)*0.1\n",
    "        translate_range_y = (y2-y1)*0.1\n",
    "        translate_x = int((random.random()*2-1)*translate_range_x)\n",
    "        translate_y = int((random.random()*2-1)*translate_range_y)\n",
    "#        print(\"translate x:\", translate_x, \"y:\", translate_y)\n",
    "        if translate_y < 0:\n",
    "            translate_y = -min(y1, -translate_y)\n",
    "        else:\n",
    "            translate_y = min(image_dim_dict[i][0] - 1 - y2, translate_y)\n",
    "        if translate_x < 0:\n",
    "            translate_x = -min(x1, -translate_x)\n",
    "        else:\n",
    "            translate_x = min(image_dim_dict[i][1] - 1 - x2, translate_x)\n",
    "\n",
    "        x11 = x1 + translate_x\n",
    "        y11 = y1 + translate_y\n",
    "        x22 = x2 + translate_x\n",
    "        y22 = y2 + translate_y\n",
    "\n",
    "        if y22 - y11 > x22 - x11:\n",
    "            diff = (y22 - y11 - (x22 - x11))/2.0\n",
    "            x111 = max(int(x11-diff), 0)\n",
    "            x222 = min(int(x22+diff), image_dim_dict[i][1])\n",
    "            y111 = int(y11)\n",
    "            y222 = int(y22)\n",
    "        else:\n",
    "            diff = (x22 - x11 - (y22 - y11))/2.0\n",
    "            y111 = max(int(y11-diff), 0)\n",
    "            y222 = min(int(y22+diff), image_dim_dict[i][0])\n",
    "            x111 = int(x11)\n",
    "            x222 = int(x22)\n",
    "\n",
    "\n",
    "        # randomly rotate image by -10 to 10 degrees\n",
    "        angle = (random.random()*2-1)*10\n",
    "        # randomly scale image 0.9-1.1\n",
    "        ratio = (random.random()*2-1)*0.1 + 1\n",
    "        M = cv2.getRotationMatrix2D(((x111+x222)/2,(y111+y222)/2), angle, ratio)\n",
    "        rotated_image = cv2.warpAffine(raw_image_dict[i], M, (image_dim_dict[i][1], image_dim_dict[i][0]))\n",
    "#         print('angle ', angle, 'ratio', ratio)\n",
    "#         plt.imshow(raw_image_dict[i], cmap='gray')\n",
    "#         plt.show()\n",
    "#         plt.imshow(rotated_image, cmap='gray')\n",
    "#         plt.show()\n",
    "\n",
    "        raw_image_resized = cv2.resize(raw_image_dict[i][y1:y2, x1:x2], (64, 64))\n",
    "        mod_image_resized = cv2.resize(rotated_image[y111:y222, x111:x222], (64, 64))\n",
    "#         plt.imshow(raw_image_resized, cmap='gray')\n",
    "#         plt.show()\n",
    "#         plt.imshow(mod_image_resized, cmap='gray')\n",
    "#         plt.show()\n",
    "        image_dict[i + '_augment_' + str(j)] = mod_image_resized\n",
    "\n",
    "    for j in range(71):\n",
    "        landmark_dict[i][j] = (landmark_dict[i][j] - np.array([x111, y111]))/(np.array([x222-x111, y222-y111]))*np.array([64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouth_openness = {}\n",
    "\n",
    "for i in sorted(landmark_dict.keys()):\n",
    "    center_side = max(np.linalg.norm(landmark_dict[i][44] - landmark_dict[i][60]),\n",
    "                      np.linalg.norm(landmark_dict[i][50] - landmark_dict[i][60]))\n",
    "    center_open = np.linalg.norm(landmark_dict[i][57] - landmark_dict[i][60])\n",
    "    top_open = np.linalg.norm(landmark_dict[i][47] - landmark_dict[i][53])\n",
    "\n",
    "    mouth_openness[i] = center_open/max(top_open, center_side)\n",
    "#    print(mouth_openness[i])\n",
    "#     debug_img = image_dict[i].copy()\n",
    "#     for j in mouth_landmarks:\n",
    "#         cv2.circle(debug_img, (int(landmark_dict[i][j][0]), int(landmark_dict[i][j][1])), 1, (255))\n",
    "#     plt.imshow(debug_img, cmap='gray')\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "emotion_model_path = '../tensorflow/models/emotion_mini_XCEPTION_64x64_0.66_7ms.hdf5.pb'\n",
    "graph = tf.Graph()\n",
    "graph_def = tf.GraphDef()\n",
    "with open(emotion_model_path, \"rb\") as f:\n",
    "    graph_def.ParseFromString(f.read())\n",
    "with graph.as_default():\n",
    "    tf.import_graph_def(graph_def)\n",
    "input_name = 'import/input_1'\n",
    "output_add_name = 'import/add_4/add'\n",
    "output_conv_name = 'import/conv2d_7/BiasAdd'\n",
    "output_name = 'import/output_node0'\n",
    "\n",
    "input_operation = graph.get_operation_by_name(input_name)\n",
    "output_add_operation  = graph.get_operation_by_name(output_add_name)\n",
    "output_conv_operation = graph.get_operation_by_name(output_conv_name)\n",
    "output_operation = graph.get_operation_by_name(output_name)\n",
    "\n",
    "input_shape = (int(input_operation.outputs[0].shape.dims[1]),\n",
    "               int(input_operation.outputs[0].shape.dims[2]),\n",
    "               1)\n",
    "\n",
    "add_dict = {}\n",
    "mean_of_add_dict = {}\n",
    "conv_dict = {}\n",
    "output_dict = {}\n",
    "add_sampled_dict = {}\n",
    "sess = tf.Session(graph = graph)\n",
    "for i in sorted(image_dict.keys()):\n",
    "    gray_face = (image_dict[i] - 127.5) / 127.5\n",
    "    gray_face = np.expand_dims(gray_face, 0)\n",
    "    gray_face = np.expand_dims(gray_face, -1)\n",
    "    prediction = sess.run([output_add_operation.outputs[0], output_conv_operation.outputs[0], output_operation.outputs[0]],\n",
    "                          {input_operation.outputs[0]: gray_face})\n",
    "    add_out = prediction[0][0]\n",
    "    add_out = add_out.reshape(16, 128)\n",
    "    mean_of_add_out = np.mean(add_out, axis=0)\n",
    "    add_out = add_out.reshape(2048)\n",
    "    add_dict[i] = add_out.copy()\n",
    "    add_sampled_dict[i] = [add_out[j] for j in [728, 1749, 1741, 1914]]\n",
    "    mean_of_add_dict[i] = mean_of_add_out.copy()\n",
    "    conv_dict[i] = prediction[1][0].reshape(112)\n",
    "    output_dict[i] = prediction[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15874708, 0.32331732, 0.4854281, -0.104132414]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'104810657_1ce2931c9f.jpg'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ee1a35473b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_sampled_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'104810657_1ce2931c9f.jpg_augment_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_of_add_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'104810657_1ce2931c9f.jpg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmouth_openness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_sampled_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'104810657_1ce2931c9f.jpg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '104810657_1ce2931c9f.jpg'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(add_sampled_dict['104810657_1ce2931c9f.jpg_augment_0'])\n",
    "print(mean_of_add_dict['104810657_1ce2931c9f.jpg'].shape)\n",
    "print(input_shape)\n",
    "print(len(mouth_openness.keys()))\n",
    "print(add_sampled_dict['104810657_1ce2931c9f.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size is 5110\n",
      "[-0.46462565 -0.44840826 -0.45747143]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing SVM\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "sorted_keys = sorted(image_dict.keys())\n",
    "np.random.shuffle(sorted_keys)\n",
    "# shuffle\n",
    "for i in sorted_keys:\n",
    "    X.append(list(add_sampled_dict[i]))\n",
    "    filename = i.split('_augment_')[0]\n",
    "    if mouth_openness[filename] < 0.1:\n",
    "        y.append(-1)\n",
    "    else:\n",
    "        y.append(mouth_openness[filename])\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.LinearSVR()\n",
    "total_size = len(X)\n",
    "print('total size is {}'.format(total_size))\n",
    "train_size = int(0.8*total_size)\n",
    "\n",
    "print(cross_val_score(clf, X, y, scoring='neg_mean_absolute_error'))\n",
    "#cross_val_score(clf, X, y, scoring='neg_mean_absolute_error') \n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in sorted(image_dict.keys()):\n",
    "    counter += 1\n",
    "    debug_img = image_dict[i].copy()\n",
    "#     for j in mouth_landmarks:\n",
    "#         cv2.circle(debug_img, (int(landmark_dict[i][j][0]), int(landmark_dict[i][j][1])), 1, (255))\n",
    "    filename = i.split('_augment_')[0]\n",
    "    #plt.imshow(debug_img, cmap='gray')\n",
    "    \n",
    "    #plt.title('mouth openness {}; predicted {}; {}: {}, {}: {}'.format(mouth_openness[filename], \n",
    "#                                                                        clf.predict([list(add_sampled_dict[i])]),\n",
    "#                                                                       728, add_dict[i][728],\n",
    "#                                                                       1914, -add_dict[i][1914]))\n",
    "    #plt.show()\n",
    "    if counter > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 4, 128)\n",
      "(?, 4, 4, 7)\n",
      "(?, 4, 4, 7)\n",
      "(?, 4, 4, 7)\n"
     ]
    }
   ],
   "source": [
    "print(graph.get_operation_by_name('import/add_4/add').outputs[0].shape)\n",
    "\n",
    "print(graph.get_operation_by_name('import/conv2d_7/BiasAdd').inputs[0].shape)\n",
    "print(graph.get_operation_by_name('import/conv2d_7/BiasAdd').outputs[0].shape)\n",
    "print(graph.get_operation_by_name('import/global_average_pooling2d_1/Mean').inputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clf.intercept_)\n",
    "# print(list(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'import/input_1' type=Placeholder>,\n",
       " <tf.Operation 'import/conv2d_1/kernel' type=Const>,\n",
       " <tf.Operation 'import/conv2d_1/kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/conv2d_1/convolution' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_1/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_1/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_1/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_1/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_1/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_1/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_1/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_1/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_1/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_1/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_1/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_1/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_1/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_1/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_1/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_1/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/activation_1/Relu' type=Relu>,\n",
       " <tf.Operation 'import/conv2d_2/kernel' type=Const>,\n",
       " <tf.Operation 'import/conv2d_2/kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/conv2d_2/convolution' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_2/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_2/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_2/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_2/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_2/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_2/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_2/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_2/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_2/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_2/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_2/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_2/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_2/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_2/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_2/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_2/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/activation_2/Relu' type=Relu>,\n",
       " <tf.Operation 'import/separable_conv2d_1/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_1/depthwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_1/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_1/pointwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_1/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/separable_conv2d_1/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_4/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_4/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_4/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_4/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_4/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_4/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_4/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_4/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_4/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_4/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_4/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_4/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_4/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_4/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_4/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_4/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/activation_3/Relu' type=Relu>,\n",
       " <tf.Operation 'import/separable_conv2d_2/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_2/depthwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_2/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_2/pointwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_2/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/separable_conv2d_2/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_5/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_5/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_5/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_5/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_5/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_5/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_5/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_5/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_5/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_5/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_5/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_5/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_5/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_5/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_5/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_5/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/conv2d_3/kernel' type=Const>,\n",
       " <tf.Operation 'import/conv2d_3/kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/conv2d_3/convolution' type=Conv2D>,\n",
       " <tf.Operation 'import/max_pooling2d_1/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'import/batch_normalization_3/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_3/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_3/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_3/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_3/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_3/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_3/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_3/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_3/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_3/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_3/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_3/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_3/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_3/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_3/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_3/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/add_1/add' type=Add>,\n",
       " <tf.Operation 'import/separable_conv2d_3/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_3/depthwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_3/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_3/pointwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_3/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/separable_conv2d_3/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_7/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_7/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_7/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_7/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_7/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_7/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_7/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_7/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_7/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_7/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_7/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_7/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_7/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_7/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_7/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_7/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/activation_4/Relu' type=Relu>,\n",
       " <tf.Operation 'import/separable_conv2d_4/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_4/depthwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_4/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_4/pointwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_4/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/separable_conv2d_4/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_8/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_8/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_8/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_8/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_8/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_8/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_8/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_8/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_8/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_8/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_8/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_8/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_8/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_8/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_8/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_8/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/conv2d_4/kernel' type=Const>,\n",
       " <tf.Operation 'import/conv2d_4/kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/conv2d_4/convolution' type=Conv2D>,\n",
       " <tf.Operation 'import/max_pooling2d_2/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'import/batch_normalization_6/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_6/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_6/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_6/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_6/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_6/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_6/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_6/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_6/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_6/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_6/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_6/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_6/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_6/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_6/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_6/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/add_2/add' type=Add>,\n",
       " <tf.Operation 'import/separable_conv2d_5/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_5/depthwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_5/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_5/pointwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_5/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/separable_conv2d_5/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_10/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_10/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_10/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_10/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_10/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_10/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_10/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_10/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_10/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_10/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_10/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_10/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_10/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_10/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_10/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_10/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/activation_5/Relu' type=Relu>,\n",
       " <tf.Operation 'import/separable_conv2d_6/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_6/depthwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_6/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_6/pointwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_6/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/separable_conv2d_6/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_11/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_11/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_11/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_11/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_11/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_11/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_11/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_11/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_11/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_11/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_11/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_11/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_11/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_11/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_11/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_11/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/conv2d_5/kernel' type=Const>,\n",
       " <tf.Operation 'import/conv2d_5/kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/conv2d_5/convolution' type=Conv2D>,\n",
       " <tf.Operation 'import/max_pooling2d_3/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'import/batch_normalization_9/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_9/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_9/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_9/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_9/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_9/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_9/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_9/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_9/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_9/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_9/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_9/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_9/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_9/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_9/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_9/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/add_3/add' type=Add>,\n",
       " <tf.Operation 'import/separable_conv2d_7/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_7/depthwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_7/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_7/pointwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_7/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/separable_conv2d_7/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_13/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_13/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_13/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_13/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_13/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_13/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_13/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_13/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_13/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_13/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_13/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_13/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_13/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_13/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_13/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_13/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/activation_6/Relu' type=Relu>,\n",
       " <tf.Operation 'import/separable_conv2d_8/depthwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_8/depthwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_8/pointwise_kernel' type=Const>,\n",
       " <tf.Operation 'import/separable_conv2d_8/pointwise_kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/separable_conv2d_8/separable_conv2d/depthwise' type=DepthwiseConv2dNative>,\n",
       " <tf.Operation 'import/separable_conv2d_8/separable_conv2d' type=Conv2D>,\n",
       " <tf.Operation 'import/batch_normalization_14/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_14/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_14/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_14/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_14/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_14/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_14/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_14/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_14/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_14/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_14/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_14/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_14/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_14/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_14/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_14/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/conv2d_6/kernel' type=Const>,\n",
       " <tf.Operation 'import/conv2d_6/kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/conv2d_6/convolution' type=Conv2D>,\n",
       " <tf.Operation 'import/max_pooling2d_4/MaxPool' type=MaxPool>,\n",
       " <tf.Operation 'import/batch_normalization_12/gamma' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_12/gamma/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_12/beta' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_12/beta/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_12/moving_mean' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_12/moving_mean/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_12/moving_variance' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_12/moving_variance/read' type=Identity>,\n",
       " <tf.Operation 'import/batch_normalization_12/batchnorm/add/y' type=Const>,\n",
       " <tf.Operation 'import/batch_normalization_12/batchnorm/add' type=Add>,\n",
       " <tf.Operation 'import/batch_normalization_12/batchnorm/Rsqrt' type=Rsqrt>,\n",
       " <tf.Operation 'import/batch_normalization_12/batchnorm/mul' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_12/batchnorm/mul_1' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_12/batchnorm/mul_2' type=Mul>,\n",
       " <tf.Operation 'import/batch_normalization_12/batchnorm/sub' type=Sub>,\n",
       " <tf.Operation 'import/batch_normalization_12/batchnorm/add_1' type=Add>,\n",
       " <tf.Operation 'import/add_4/add' type=Add>,\n",
       " <tf.Operation 'import/conv2d_7/kernel' type=Const>,\n",
       " <tf.Operation 'import/conv2d_7/kernel/read' type=Identity>,\n",
       " <tf.Operation 'import/conv2d_7/bias' type=Const>,\n",
       " <tf.Operation 'import/conv2d_7/bias/read' type=Identity>,\n",
       " <tf.Operation 'import/conv2d_7/convolution' type=Conv2D>,\n",
       " <tf.Operation 'import/conv2d_7/BiasAdd' type=BiasAdd>,\n",
       " <tf.Operation 'import/global_average_pooling2d_1/Mean/reduction_indices' type=Const>,\n",
       " <tf.Operation 'import/global_average_pooling2d_1/Mean' type=Mean>,\n",
       " <tf.Operation 'import/predictions/Softmax' type=Softmax>,\n",
       " <tf.Operation 'import/output_node0' type=Identity>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5110"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.intercept_)\n",
    "print(list(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
